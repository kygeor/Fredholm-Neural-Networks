# -*- coding: utf-8 -*-
"""potential_fredholm_nn_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gcxIRBs5FwAYUdYULhvM1nR8kLR9QbDP
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

#  FredholmNeuralNetwork and Potential Fredholm NN class definitions for the application to elliptic PDEs

# Class that is used for the Boundary Integral Equation via the Fredholm NN for the Poisson PDE

class FredholmNeuralNetwork_Poisson(nn.Module):
    def __init__(self, grid_dictionary, kernel, grid_step, K, input_size, output_size, km_constant, func_fn, precomputed_integral_map):
        super(FredholmNeuralNetwork_Poisson, self).__init__()

        self.grid_dictionary = grid_dictionary
        self.kernel = kernel
        self.grid_step = grid_step
        self.K = K
        self.km_constant = km_constant
        self.func_fn = func_fn
        self.precomputed_integral_map = precomputed_integral_map

        # Store dimensions for each layer
        self.layer_sizes = [input_size] + [len(grid_dictionary[f'layer_{i}']) for i in range(K + 1)]

    def additive(self, out_values):
        """Fetch precomputed additive values for x_list."""
        out_values = out_values.numpy() if isinstance(out_values, torch.Tensor) else out_values
        additive_values = []

        # Convert the precomputed integral map into a sorted array for fast lookup
        precomputed_grid = np.array(list(self.precomputed_integral_map.keys()))
        precomputed_values = np.array(list(self.precomputed_integral_map.values()))

        for value in out_values:
            # Find the closest grid point to the current value
            idx = np.abs(precomputed_grid - value).argmin()
            closest_value = precomputed_values[idx]

            # Compute the additive term
            additive_values.append(2 * (self.func_fn(value) - closest_value))

        return np.array(additive_values)

    def compute_weights_and_biases(self):
        """Precomputes weights and biases based on the grid using vectorized operations."""
        weights = []
        biases = []

        for i in range(self.K + 1):
            if i == 0:
                # Boundary condition for the first layer
                mat = np.identity(self.layer_sizes[i])
                weights.append(torch.tensor(mat, dtype=torch.float32))
                biases.append(torch.zeros(self.layer_sizes[i], dtype=torch.float32))
            else:
                # Vectorized kernel computation for weight matrix
                grid_prev = np.expand_dims(self.grid_dictionary[f'layer_{i - 1}'], axis=1)  # [N, 1]
                grid_curr = np.expand_dims(self.grid_dictionary[f'layer_{i}'], axis=0)    # [1, M]

                weight_matrix = (
                    self.kernel(grid_prev, grid_curr) * self.grid_step * self.km_constant +
                    (1 - self.km_constant) * ((grid_prev - grid_curr) == 0.0)
                )  # [N, M]
                weights.append(torch.tensor(weight_matrix, dtype=torch.float32))

                # Squeeze grid_curr from shape [1, M] to [M]
                grid_curr_squeezed = grid_curr.squeeze(0)  # shape [M]

                # Compute the additive term => shape (M,)
                bias_vector = self.additive(grid_curr_squeezed)*self.km_constant
                biases.append(torch.tensor(bias_vector, dtype=torch.float32))

        return weights, biases

    def forward(self, predict_array):
        # Precompute weights and biases
        weights, biases = self.compute_weights_and_biases()

        # Pass input through the dynamically updated network
        x = torch.ones(len(self.grid_dictionary['layer_0']), dtype=torch.float32)
        for i in range(self.K + 1):
            x = torch.matmul(weights[i].T, x) + biases[i]

        # Compute the last hidden layer output
        prediction = x

        return prediction.squeeze()

## Class for the Potential Fredholm NN to solve the Poisson PDE

class PotentialFredholmNeuralNetwork_Poisson(nn.Module):
    def __init__(self, fredholm_model, diff_potentials_fn, potential_boundary_fn, precomputed_integral_out, plot_BIF=False):
        super(PotentialFredholmNeuralNetwork_Poisson, self).__init__()
        self.fredholm_model = fredholm_model
        self.diff_potentials_fn = diff_potentials_fn
        self.potential_boundary_fn = potential_boundary_fn
        self.precomputed_integral_out = precomputed_integral_out
        self.plot_BIF = plot_BIF

    def forward(self, input, r_out, theta_out, phi_grid, grid_step):
        phi_grid = torch.tensor(phi_grid, dtype=torch.float32) if isinstance(phi_grid, np.ndarray) else phi_grid
        theta_out = torch.tensor(theta_out, dtype=torch.float32) if isinstance(theta_out, np.ndarray) else theta_out

        fnn_output = self.fredholm_model(input).to(dtype=torch.float32)

        if self.plot_BIF:
          plt.figure(figsize=(5, 4))
          plt.plot(input.numpy(), fnn_output.detach().numpy(), label="Model Output")
          plt.xlabel("Predict Array (x_list)")
          plt.ylabel("Output")
          plt.title("Boundary function")
          plt.legend()
          plt.grid(True)
          plt.show()

        # Additional hidden layer
        hidden_weights = torch.eye(len(phi_grid), dtype=torch.float32) # Construct Identity matrix for the weights in the additional hidden layer
        theta_indices = torch.argmin(torch.abs(phi_grid.unsqueeze(0) - theta_out.unsqueeze(-1)), dim=-1) # find corresponding Î²(x*) for substraction below
        fnn_output_theta = fnn_output[theta_indices]
        fnn_output_theta_expanded = fnn_output_theta.unsqueeze(0).repeat(len(r_out), 1).unsqueeze(-1)
        hidden_bias = -fnn_output_theta_expanded
        hidden_output = fnn_output.unsqueeze(0).unsqueeze(0) + hidden_bias

        # Output layer - r_out and theta_out are "tensored" via the potential and diff potential functions
        output_weights = torch.tensor(self.diff_potentials_fn(phi_grid, r_out, theta_out) * grid_step, dtype=torch.float32) # Multiply by the difference in potentials for the limit-approaching term
        potential_boundary_term = self.potential_boundary_fn(phi_grid, r_out, theta_out)

        bias_output = (
            torch.sum(fnn_output.unsqueeze(0).unsqueeze(0) * potential_boundary_term, dim=-1) * grid_step +
            0.5 * fnn_output_theta.unsqueeze(0).repeat(len(r_out), 1)) # Calculate the first part of the bias term as the integral of the potential kernel*boundary function

        poisson_integral = self.precomputed_integral_out
        poisson_integral = torch.tensor(poisson_integral, dtype = torch.float32)
        print(bias_output.shape)
        print(poisson_integral.shape)
        bias_output += poisson_integral  # Add precomputed integral of fundamental*source for each (r_o, theta_o)

        output_weights = output_weights.unsqueeze(-1)
        final_output = torch.matmul(hidden_output.unsqueeze(-2), output_weights).squeeze(-2)
        final_output += bias_output.unsqueeze(-1)
        return final_output

# Class that is used for the Boundary Integral Equation via the Fredholm NN for the Helmholtz PDE

class FredholmNeuralNetwork_Helm(nn.Module):
    def __init__(self, grid_dictionary, kernel, grid_step, K, input_size, output_size, km_constant, func_fn, precomputed_integral_map):
        super(FredholmNeuralNetwork_Helm, self).__init__()

        self.grid_dictionary = grid_dictionary
        self.kernel = kernel
        self.grid_step = grid_step
        self.K = K
        self.km_constant = km_constant
        self.func_fn = func_fn
        self.precomputed_integral_map = precomputed_integral_map

        # Store dimensions for each layer
        self.layer_sizes = [input_size] + [len(grid_dictionary[f'layer_{i}']) for i in range(K + 1)]

    def additive(self, out_values):
        """Fetch precomputed additive values for x_list."""
        out_values = out_values.numpy() if isinstance(out_values, torch.Tensor) else out_values
        additive_values = []

        # Convert the precomputed integral map into a sorted array for fast lookup
        precomputed_grid = np.array(list(self.precomputed_integral_map.keys()))
        precomputed_values = np.array(list(self.precomputed_integral_map.values()))

        for value in out_values:
            # Find the closest grid point to the current value
            idx = np.abs(precomputed_grid - value).argmin()
            closest_value = precomputed_values[idx]

            # Compute the additive term
            additive_values.append(2 * (self.func_fn(value) - closest_value))

        return np.array(additive_values)

    def compute_weights_and_biases(self):
        """Precomputes weights and biases based on the grid using vectorized operations."""
        weights = []
        biases = []

        for i in range(self.K + 1):
            if i == 0:
                # Boundary condition for the first layer
                mat = np.identity(self.layer_sizes[i])
                weights.append(torch.tensor(mat, dtype=torch.float32))
                biases.append(torch.zeros(self.layer_sizes[i], dtype=torch.float32))
            else:
                # Vectorized kernel computation for weight matrix
                grid_prev = np.expand_dims(self.grid_dictionary[f'layer_{i - 1}'], axis=1)  # [N, 1]
                grid_curr = np.expand_dims(self.grid_dictionary[f'layer_{i}'], axis=0)    # [1, M]

                weight_matrix = (
                    self.kernel(grid_prev, grid_curr) * self.grid_step * self.km_constant +
                    (1 - self.km_constant) * ((grid_prev - grid_curr) == 0.0)
                )  # [N, M]
                weights.append(torch.tensor(weight_matrix, dtype=torch.float32))

                # Squeeze grid_curr from shape [1, M] to [M]
                grid_curr_squeezed = grid_curr.squeeze(0)  # shape [M]

                # Compute the additive term => shape (M,)
                bias_vector = self.additive(grid_curr_squeezed)*self.km_constant
                biases.append(torch.tensor(bias_vector, dtype=torch.float32))


        return weights, biases

    def forward(self, predict_array):
        # Precompute weights and biases
        weights, biases = self.compute_weights_and_biases()

        # Pass input through the dynamically updated network
        x = torch.ones(len(self.grid_dictionary['layer_0']), dtype=torch.float32)
        for i in range(self.K + 1):
            x = torch.matmul(weights[i].T, x) + biases[i]


        prediction = x

        return prediction.squeeze()

# Class for the Potential Fredholm NN to solve the Helmholtz PDE with precomputed fundamental*source and lambda*fundamental integrals

class PotentialFredholmNeuralNetwork_Helm(torch.nn.Module):
    def __init__(
        self,
        fredholm_model,
        diff_potentials_fn,
        potential_boundary_fn,
        precomputed_integral_entire_domain_source,
        precomputed_integral_entire_domain_fund,
        lambda_value=1.0,
        plot_BIF = False
    ):

        super().__init__()
        self.fredholm_model = fredholm_model
        self.diff_potentials_fn = diff_potentials_fn
        self.potential_boundary_fn = potential_boundary_fn
        self.precomputed_integral_entire_domain_source = precomputed_integral_entire_domain_source
        self.precomputed_integral_entire_domain_fund = precomputed_integral_entire_domain_fund
        self.lambda_value = lambda_value
        self.plot_BIF = plot_BIF


    def forward(self, input, r_out, theta_out, phi_grid, grid_step):
        # 1) Convert if needed
        if isinstance(phi_grid, np.ndarray):
            phi_grid = torch.tensor(phi_grid, dtype=torch.float32)
        if isinstance(theta_out, np.ndarray):
            theta_out = torch.tensor(theta_out, dtype=torch.float32)

        # 2) Evaluate boundary function
        fnn_output = self.fredholm_model(input).float()

        if self.plot_BIF:
          # (Optional) Plot
          plt.figure(figsize=(5, 4))
          plt.plot(input.numpy(), fnn_output.detach().numpy(), label="Model Output")
          plt.xlabel("Predict Array (x_list)")
          plt.ylabel("Output")
          plt.title("Boundary function")
          plt.legend()
          plt.grid(True)
          plt.show()

        # Build hidden output logic
        theta_indices = torch.argmin(torch.abs(phi_grid.unsqueeze(0) - theta_out.unsqueeze(-1)), dim=-1)
        fnn_output_theta = fnn_output[theta_indices]  # shape [len(theta_out)]

        r_out_torch = torch.tensor(r_out, dtype=torch.float32)
        # shape => [R,T,1]
        fnn_output_theta_expanded = fnn_output_theta.unsqueeze(0).repeat(len(r_out), 1).unsqueeze(-1)

        hidden_bias = -fnn_output_theta_expanded
        hidden_output = fnn_output.unsqueeze(0).unsqueeze(0) + hidden_bias

        # Evaluate diff potentials and boundary
        output_weights = torch.tensor(self.diff_potentials_fn(phi_grid, r_out, theta_out) * grid_step, dtype=torch.float32)  # shape [R,T, N_phi]
        potential_boundary_term = self.potential_boundary_fn(phi_grid, r_out, theta_out)  # shape [R,T, N_phi]

        # sum over phi dim => shape [R,T]
        bias_output = (torch.sum(fnn_output.unsqueeze(0).unsqueeze(0) * potential_boundary_term, dim=-1) * grid_step)

        # Call integrals for output bias construction
        integral_fund_source = torch.tensor(self.precomputed_integral_entire_domain_source, dtype = torch.float32)
        integral_sqrtlambda_fund = torch.tensor(self.precomputed_integral_entire_domain_fund, dtype = torch.float32)
        #integral_fund_source = integral_fund_source.squeeze(-1) # shape => [R,T]

        # Add the integral_fund_source to bias_output
        bias_output += integral_fund_source

        # Incorporate the second integral for factor = 0.5 + [ integral_sqrtlambda_fund(r_out,theta_out) - integral_sqrtlambda_fund(1.0,theta_out) ]
        boundary_vals = integral_sqrtlambda_fund[-1, :]

        # broadcast difference
        sqrtlambda_diff = integral_sqrtlambda_fund - boundary_vals.unsqueeze(0)

        factor = 0.5 + sqrtlambda_diff  # shape [R,T]

        # multiply by fnn_output_theta_expanded.squeeze(-1)
        half_term_corrected = factor.squeeze(-1) * fnn_output_theta_expanded.squeeze(-1)  # [R,T]

        # 6) Add half_term_corrected to bias_output
        bias_output += half_term_corrected

        # Calculate final output
        output_weights = output_weights.unsqueeze(-1)
        final_output = torch.matmul(hidden_output.unsqueeze(-2), output_weights).squeeze(-2)

        # Add bias
        final_output += bias_output.unsqueeze(-1)

        return final_output

## Class for the Potential Fredholm NN to solve the semi-linear elliptic PDE

class FredholmNeuralNetwork_semi_linear(nn.Module):
    def __init__(self, grid_dictionary, kernel, grid_step, K, input_size, output_size, km_constant, func_fn, precomputed_integral_map):
        super(FredholmNeuralNetwork_semi_linear, self).__init__()

        self.grid_dictionary = grid_dictionary
        self.kernel = kernel
        self.grid_step = grid_step
        self.K = K
        self.km_constant = km_constant
        self.func_fn = func_fn
        self.precomputed_integral_map = precomputed_integral_map

        # Store dimensions for each layer
        self.layer_sizes = [input_size] + [len(grid_dictionary[f'layer_{i}']) for i in range(K + 1)]

    def additive(self, out_values):
        """Fetch precomputed additive values for x_list."""
        out_values = out_values.numpy() if isinstance(out_values, torch.Tensor) else out_values
        additive_values = []

        # Convert the precomputed integral map into a sorted array for fast lookup
        precomputed_grid = np.array(list(self.precomputed_integral_map.keys()))
        precomputed_values = np.array(list(self.precomputed_integral_map.values()))

        for value in out_values:
            # Find the closest grid point to the current value
            idx = np.abs(precomputed_grid - value).argmin()
            closest_value = precomputed_values[idx]

            # Compute the additive term
            additive_values.append(2 * (self.func_fn(value) - closest_value))

        return np.array(additive_values)

    def compute_weights_and_biases(self):
        """Precomputes weights and biases based on the grid using vectorized operations."""
        weights = []
        biases = []

        for i in range(self.K + 1):
            if i == 0:
                # Boundary condition for the first layer
                mat = np.identity(self.layer_sizes[i])
                weights.append(torch.tensor(mat, dtype=torch.float32))
                biases.append(torch.zeros(self.layer_sizes[i], dtype=torch.float32))
            else:
                # Vectorized kernel computation for weight matrix
                grid_prev = np.expand_dims(self.grid_dictionary[f'layer_{i - 1}'], axis=1)  # [N, 1]
                grid_curr = np.expand_dims(self.grid_dictionary[f'layer_{i}'], axis=0)    # [1, M]

                weight_matrix = (
                    self.kernel(grid_prev, grid_curr) * self.grid_step * self.km_constant +
                    (1 - self.km_constant) * ((grid_prev - grid_curr) == 0.0)
                )  # [N, M]
                weights.append(torch.tensor(weight_matrix, dtype=torch.float32))

                # Squeeze grid_curr from shape [1, M] to [M]
                grid_curr_squeezed = grid_curr.squeeze(0)  # shape [M]

                # Compute the additive term => shape (M,)
                bias_vector = self.additive(grid_curr_squeezed)*self.km_constant
                biases.append(torch.tensor(bias_vector, dtype=torch.float32))


        return weights, biases

    def forward(self, predict_array):
        # Precompute weights and biases
        weights, biases = self.compute_weights_and_biases()

        # Pass input through the dynamically updated network
        x = torch.ones(len(self.grid_dictionary['layer_0']), dtype=torch.float32)
        for i in range(self.K + 1):
            x = torch.matmul(weights[i].T, x) + biases[i]

        # Compute the last hidden layer output
        nn_output = x

        # grid = self.grid_dictionary[f'layer_{len(self.grid_dictionary) - 1}']

        # # Final prediction based on predict_array
        # weights_K_array = []
        # predict_tensor = torch.tensor(predict_array, dtype=torch.float32)
        # grid_tensor = torch.tensor(grid, dtype=torch.float32)

        # for predict_value in predict_tensor:
        #     weights_K = torch.tensor([
        #         self.kernel(grid_value, predict_value.item()) * self.grid_step
        #         for grid_value in grid_tensor
        #     ], dtype=torch.float32).view(1, -1)
        #     weights_K_array.append(weights_K)

        # weights_K_array = torch.cat(weights_K_array, dim=0)
        # bias_K_array = self.additive(predict_tensor)

        # prediction = torch.matmul(weights_K_array, nn_output.T) + bias_K_array

        # Compute the last hidden layer output
        # nn_output = x
        # print(nn_output.shape)

        prediction = nn_output

        return prediction.squeeze()

# Class for the Potential Fredholm NN to solve the semi-linear elliptic PDE with precomputed fundamental*source and lambda*fundamental integrals

class PotentialFredholmNeuralNetwork_semi_linear(torch.nn.Module):
    def __init__(
        self,
        fredholm_model,
        diff_potentials_fn,
        potential_boundary_fn,
        precomputed_integral_entire_domain_source,
        precomputed_integral_entire_domain_fund,
        lambda_value=1.0,
        plot_BIF = False):

        super().__init__()
        self.fredholm_model = fredholm_model
        self.diff_potentials_fn = diff_potentials_fn
        self.potential_boundary_fn = potential_boundary_fn
        self.precomputed_integral_entire_domain_source = precomputed_integral_entire_domain_source
        self.precomputed_integral_entire_domain_fund = precomputed_integral_entire_domain_fund
        self.plot_BIF = plot_BIF
        self.lambda_value = lambda_value


    def forward(self, input, r_out, theta_out, phi_grid, grid_step):
        # 1) Convert if needed
        if isinstance(phi_grid, np.ndarray):
            phi_grid = torch.tensor(phi_grid, dtype=torch.float32)
        if isinstance(theta_out, np.ndarray):
            theta_out = torch.tensor(theta_out, dtype=torch.float32)

        # 2) Evaluate boundary function
        fnn_output = self.fredholm_model(input).float()
        print(np.max(np.abs(fnn_output.detach().numpy())))

        if self.plot_BIF:
          # (Optional) Plot
          plt.figure(figsize=(5, 4))
          plt.plot(input.numpy(), fnn_output.detach().numpy(), label="Model Output")
          plt.xlabel("Predict Array (x_list)")
          plt.ylabel("Output")
          plt.title("Boundary function")
          plt.legend()
          plt.grid(True)
          plt.show()

        # 3) Build hidden output logic
        theta_indices = torch.argmin(torch.abs(phi_grid.unsqueeze(0) - theta_out.unsqueeze(-1)), dim=-1)
        fnn_output_theta = fnn_output[theta_indices]  # shape [len(theta_out)]

        r_out_torch = torch.tensor(r_out, dtype=torch.float32)
        # shape => [R,T,1]
        fnn_output_theta_expanded = fnn_output_theta.unsqueeze(0).repeat(len(r_out), 1).unsqueeze(-1)

        hidden_bias = -fnn_output_theta_expanded
        hidden_output = fnn_output.unsqueeze(0).unsqueeze(0) + hidden_bias

        # 4) Evaluate diff potentials and boundary
        output_weights = torch.tensor(
            self.diff_potentials_fn(phi_grid, r_out, theta_out) * grid_step, dtype=torch.float32)  # shape [R,T, N_phi]
        potential_boundary_term = self.potential_boundary_fn(phi_grid, r_out, theta_out)  # shape [R,T, N_phi]

        # sum over phi dim => shape [R,T]
        bias_output = (torch.sum(fnn_output.unsqueeze(0).unsqueeze(0) * potential_boundary_term,dim=-1) * grid_step)

        # Call integrals for bias construction
        integral_fund_source = self.precomputed_integral_entire_domain_source
        integral_sqrtlambda_fund = torch.tensor(self.precomputed_integral_entire_domain_fund, dtype = torch.float32)
        integral_fund_source = integral_fund_source.squeeze(-1) # shape => [R,T]

        # Add the integral_fund_source to bias_output
        bias_output += integral_fund_source

        # Incorporate the second integral:
        boundary_vals = integral_sqrtlambda_fund[-1, :]  # shape [T]
        # broadcast difference
        sqrtlambda_diff = integral_sqrtlambda_fund - boundary_vals.unsqueeze(0)

        factor = 0.5 + sqrtlambda_diff  # shape [R,T]

        # multiply by fnn_output_theta_expanded.squeeze(-1)
        half_term_corrected = factor.squeeze(-1) * fnn_output_theta_expanded.squeeze(-1)  # [R,T]

        # Add half_term_corrected to bias_output
        bias_output += half_term_corrected

        # Calculate final output
        output_weights = output_weights.unsqueeze(-1)  # => [R,T,N_phi,1]
        # hidden_output => shape [1,1,N_phi], need broadcast
        final_output = torch.matmul(hidden_output.unsqueeze(-2), output_weights).squeeze(-2)
        # => shape [R,T,1]

        # add bias => shape [R,T,1]
        final_output += bias_output.unsqueeze(-1)

        return final_output
